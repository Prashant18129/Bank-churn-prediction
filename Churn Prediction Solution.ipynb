{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Prediction using Logisitic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "There are multiple variables in the dataset which can be cleanly divided in 3 categories:\n",
    "\n",
    "### Demographic information about customers\n",
    "\n",
    "<b>customer_id</b> - Customer id\n",
    "\n",
    "<b>vintage</b> - Vintage of the customer with the bank in number of days\n",
    "\n",
    "<b>age</b> - Age of customer\n",
    "\n",
    "<b>gender</b> - Gender of customer\n",
    "\n",
    "<b>dependents</b> - Number of dependents\n",
    "\n",
    "<b>occupation</b> - Occupation of the customer \n",
    "\n",
    "<b>city</b> - City of customer (anonymised)\n",
    "\n",
    "\n",
    "### Customer Bank Relationship\n",
    "\n",
    "\n",
    "<b>customer_nw_category</b> - Net worth of customer (3:Low 2:Medium 1:High)\n",
    "\n",
    "<b>branch_code</b> - Branch Code for customer account\n",
    "\n",
    "<b>days_since_last_transaction</b> - No of Days Since Last Credit in Last 1 year\n",
    "\n",
    "\n",
    "### Transactional Information\n",
    "\n",
    "<b>current_balance</b> - Balance as of today\n",
    "\n",
    "<b>previous_month_end_balance</b> - End of Month Balance of previous month\n",
    "\n",
    "\n",
    "<b>average_monthly_balance_prevQ</b> - Average monthly balances (AMB) in Previous Quarter\n",
    "\n",
    "<b>average_monthly_balance_prevQ2</b> - Average monthly balances (AMB) in previous to previous quarter\n",
    "\n",
    "<b>current_month_credit</b> - Total Credit Amount current month\n",
    "\n",
    "<b>previous_month_credit</b> - Total Credit Amount previous month\n",
    "\n",
    "<b>current_month_debit</b> - Total Debit Amount current month\n",
    "\n",
    "<b>previous_month_debit</b> - Total Debit Amount previous month\n",
    "\n",
    "<b>current_month_balance</b> - Average Balance of current month\n",
    "\n",
    "<b>previous_month_balance</b> - Average Balance of previous month\n",
    "\n",
    "<b>churn</b> - Average balance of customer falls below minimum balance in the next quarter (1/0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Prediction\n",
    "\n",
    "\n",
    "* Load Data & Packages for model building & preprocessing\n",
    "* Preprocessing & Missing value imputation\n",
    "* Select features on the basis of EDA Conclusions & build baseline model\n",
    "* Decide Evaluation Metric on the basis of business problem\n",
    "* Build model using all features & compare with baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve, precision_score, recall_score, precision_recall_curve\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('churn_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "Before we go on to build the model, we must look for missing values within the dataset as treating the missing values  is a necessary step before we fit a model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(df).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this function shows that there are quite a few missing values in columns gender, dependents, city, days since last transaction and Percentage change in credits. Let us go through each of them 1 by 1 to find the appropriate missing value imputation strategy for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender\n",
    "\n",
    "L\n",
    "et us look at the categories within gender column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is a good mix of males and females and arguably missing values cannot be filled with any one of them. We could create a seperate category by assigning the value -1 for all missing values in this column.\n",
    "\n",
    "Before that, first we will convert the gender into 0/1 and then replace missing values with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Gender\n",
    "dict_gender = {'Male': 1, 'Female':0}\n",
    "df.replace({'gender': dict_gender}, inplace = True)\n",
    "\n",
    "df['gender'] = df['gender'].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependents, occupation and city with mode\n",
    "\n",
    "Next we will have a quick look at the dependents & occupations column and impute with mode as this is sort of an ordinal variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dependents'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dependents'] = df['dependents'].fillna(0)\n",
    "df['occupation'] = df['occupation'].fillna('self_employed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly City can also be imputed with most common category 1020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city'] = df['city'].fillna(1020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Days since Last Transaction\n",
    "A fair assumption can be made on this column as this is number of days since last transaction in 1 year, we can substitute missing values with a value greater than 1 year say 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['days_since_last_transaction'] = df['days_since_last_transaction'].fillna(999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Now, before applying linear model such as logistic regression, we need to scale the data and keep all features as numeric strictly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummies with Multiple Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert occupation to one hot encoded features\n",
    "df = pd.concat([df,pd.get_dummies(df['occupation'],prefix = str('occupation'),prefix_sep='_')],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Numerical Features for Logistic Regression\n",
    "\n",
    "Now, we remember that there are a lot of outliers in the dataset especially when it comes to previous and current balance features. Also, the distributions are skewed for these features. We will take 2 steps to deal with that here:\n",
    "* Log Transformation\n",
    "* Standard Scaler\n",
    "\n",
    "Standard scaling is anyways a necessity when it comes to linear models and we have done that here after doing log transformation on all balance features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['customer_nw_category', 'current_balance',\n",
    "            'previous_month_end_balance', 'average_monthly_balance_prevQ2', 'average_monthly_balance_prevQ',\n",
    "            'current_month_credit','previous_month_credit', 'current_month_debit', \n",
    "            'previous_month_debit','current_month_balance', 'previous_month_balance']\n",
    "for i in num_cols:\n",
    "    df[i] = np.log(df[i] + 17000)\n",
    "\n",
    "std = StandardScaler()\n",
    "scaled = std.fit_transform(df[num_cols])\n",
    "scaled = pd.DataFrame(scaled,columns=num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_df_og = df.copy()\n",
    "df = df.drop(columns = num_cols,axis = 1)\n",
    "df = df.merge(scaled,left_index=True,right_index=True,how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = df.churn\n",
    "df = df.drop(['churn','customer_id','occupation'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Evaluation Metrics\n",
    "Since this is a binary classification problem, we could use the following 2 popular metrics:\n",
    "\n",
    "1. Recall\n",
    "2. Area under the Receiver operating characteristic curve\n",
    "\n",
    "Now, we are looking at the recall value here because a customer falsely marked as churn would not be as bad as a customer who was not detected as a churning customer and appropriate measures were not taken by the bank to stop him/her from churning\n",
    "\n",
    "The ROC AUC is the area under the curve when plotting the (normalized) true positive rate (x-axis) and the false positive rate (y-axis).\n",
    "\n",
    "Our main metric here would be Recall values, while AUC ROC Score would take care of how well predicted probabilites are able to differentiate between the 2 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions from EDA\n",
    "* For debit values, we see that there is a significant difference in the distribution for churn and non churn and it might be turn out to be an important feature\n",
    "* For all the balance features the lower values have much higher proportion of churning customers\n",
    "* For most frequent vintage values, the churning customers are slightly higher, while for higher values of vintage, we have mostly non churning customers which is in sync with the age variable \n",
    "* We see significant difference for different occupations and certainly would be interesting to use as a feature for prediction of churn.\n",
    "\n",
    "Now, we will first split our dataset into test and train and using the above conclusions select columns and build a baseline logistic regression model to check the ROC-AUC Score & the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_cols = ['current_month_debit', 'previous_month_debit','current_balance','previous_month_end_balance','vintage'\n",
    "                 ,'occupation_retired', 'occupation_salaried','occupation_self_employed', 'occupation_student']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = df[baseline_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split to create a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into Train and Validation set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(df_baseline,y_all,test_size=1/3, random_state=11, stratify = y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(xtrain,ytrain)\n",
    "pred = model.predict_proba(xtest)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC ROC Curve & Confusion Matrix \n",
    "\n",
    "Now, let us quickly look at the AUC-ROC curve for our logistic regression model and also the confusion matrix to see where the logistic regression model is failing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, _ = roc_curve(ytest,pred) \n",
    "auc = roc_auc_score(ytest, pred) \n",
    "plt.figure(figsize=(12,8)) \n",
    "plt.plot(fpr,tpr,label=\"Validation AUC-ROC=\"+str(auc)) \n",
    "x = np.linspace(0, 1, 1000)\n",
    "plt.plot(x, x, linestyle='-')\n",
    "plt.xlabel('False Positive Rate') \n",
    "plt.ylabel('True Positive Rate') \n",
    "plt.legend(loc=4) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "pred_val = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_preds = pred_val\n",
    "\n",
    "cm = confusion_matrix(ytest,label_preds)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, normalized=True, cmap='bone'):\n",
    "    plt.figure(figsize=[7, 6])\n",
    "    norm_cm = cm\n",
    "    if normalized:\n",
    "        norm_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=['Predicted: No','Predicted: Yes'], yticklabels=['Actual: No','Actual: Yes'], cmap=cmap)\n",
    "\n",
    "plot_confusion_matrix(cm, ['No', 'Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall Score\n",
    "recall_score(ytest,pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "\n",
    "\n",
    "Cross Validation is one of the most important concepts in any type of data modelling. It simply says, try to leave a sample on which you do not train the model and test the model on this sample before finalizing the model.\n",
    "\n",
    "We divide the entire population into k equal samples. Now we train models on k-1 samples and validate on 1 sample. Then, at the second iteration we train the model with a different sample held as validation. \n",
    "\n",
    "In k iterations, we have basically built model on each sample and held each of them as validation. This is a way to reduce the selection bias and reduce the variance in prediction power.\n",
    "\n",
    "Since it builds several models on different subsets of the dataset, we can be more sure of our model performance if we use CV for testing our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_score(ml_model, rstate = 12, thres = 0.5, cols = df.columns):\n",
    "    i = 1\n",
    "    cv_scores = []\n",
    "    df1 = df.copy()\n",
    "    df1 = df[cols]\n",
    "    \n",
    "    # 5 Fold cross validation stratified on the basis of target\n",
    "    kf = StratifiedKFold(n_splits=5,random_state=rstate,shuffle=True)\n",
    "    for df_index,test_index in kf.split(df1,y_all):\n",
    "        print('\\n{} of kfold {}'.format(i,kf.n_splits))\n",
    "        xtr,xvl = df1.loc[df_index],df1.loc[test_index]\n",
    "        ytr,yvl = y_all.loc[df_index],y_all.loc[test_index]\n",
    "            \n",
    "        # Define model for fitting on the training set for each fold\n",
    "        model = ml_model\n",
    "        model.fit(xtr, ytr)\n",
    "        pred_probs = model.predict_proba(xvl)\n",
    "        pp = []\n",
    "         \n",
    "        # Use threshold to define the classes based on probability values\n",
    "        for j in pred_probs[:,1]:\n",
    "            if j>thres:\n",
    "                pp.append(1)\n",
    "            else:\n",
    "                pp.append(0)\n",
    "         \n",
    "        # Calculate scores for each fold and print\n",
    "        pred_val = pp\n",
    "        roc_score = roc_auc_score(yvl,pred_probs[:,1])\n",
    "        recall = recall_score(yvl,pred_val)\n",
    "        precision = precision_score(yvl,pred_val)\n",
    "        sufix = \"\"\n",
    "        msg = \"\"\n",
    "        msg += \"ROC AUC Score: {}, Recall Score: {:.4f}, Precision Score: {:.4f} \".format(roc_score, recall,precision)\n",
    "        print(\"{}\".format(msg))\n",
    "         \n",
    "         # Save scores\n",
    "        cv_scores.append(roc_score)\n",
    "        i+=1\n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = cv_score(LogisticRegression(), cols = baseline_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try using all columns available to check if we get significant improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feat_scores = cv_score(LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some improvement in both ROC AUC Scores and Precision/Recall Scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_all_features = cv_score(RandomForestClassifier(n_estimators=100, max_depth=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Different model fold wise\n",
    "\n",
    "Let us visualise the cross validation scores for each fold for the following 3 models and observe differences:\n",
    "* Baseline Model\n",
    "* Model based on all features\n",
    "* Model based on top 10 features obtained from RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({'baseline':baseline_scores, 'all_feats': all_feat_scores, 'random_forest': rf_all_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.plot(y=[\"baseline\", \"all_feats\", \"random_forest\"], kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the random forest model is giving the best result for each fold and students are encouraged to try and fine tune the model to get the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
